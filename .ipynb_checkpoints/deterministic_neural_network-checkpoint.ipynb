{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd64276-6ce8-4c53-a548-76b9d0edd479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter('ignore')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea20b75-b98e-4f84-aabd-9209a7aebf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_weights(n_origin_neurons, n_destination_neurons):\n",
    "    return np.random.random(size=(n_destination_neurons, n_origin_neurons))\n",
    "    \n",
    "def generate_initial_biases(n_destination_neurons):\n",
    "    return np.random.random(size=(n_destination_neurons, 1))\n",
    "\n",
    "def combine_all_layers(input_layer, hidden_layers, output_layer):\n",
    "    return np.concatenate((input_layer,\n",
    "                            hidden_layers, \n",
    "                            output_layer))\n",
    "\n",
    "def generate_all_initial_weights(all_layers):\n",
    "    return [generate_initial_weights(n_origin_neurons, n_destination_neurons) for n_origin_neurons, n_destination_neurons in zip(all_layers[:-1], all_layers[1:])]\n",
    "    \n",
    "def generate_all_initial_biases(all_layers):\n",
    "    return [generate_initial_biases(n_destination_neurons) for n_destination_neurons in all_layers[1:]]        \n",
    "\n",
    "def feed_forward__neural_network(weights, biases, feature_data):\n",
    "    neuron_values = [feature_data]\n",
    "    for w, b in zip(weights, biases):\n",
    "        layers = np.matmul(w, neuron_values[-1]) + b\n",
    "        neuron_values.append(layers)\n",
    "\n",
    "    return neuron_values\n",
    "\n",
    "def _error_on_output_layer(target_data, layers):\n",
    "    return (target_data - layers[-1]) / (2 * len(layers[-1]))\n",
    "\n",
    "def _error_on_hidden_layers(error_on_upcoming_layer, layer, weights):\n",
    "    return np.multiply(layer, np.matmul(weights.T, error_on_upcoming_layer))\n",
    "\n",
    "def _derivative_of_cost_with_respect_to_biases(error_on_current_layer):\n",
    "    return error_on_current_layer\n",
    "\n",
    "def _derivative_of_cost_with_respect_to_weights(error_on_current_layer, previous_hidden_layer):\n",
    "    return np.matmul(error_on_current_layer, previous_hidden_layer.T)\n",
    "\n",
    "def backward_propagation_neural_network(target_data, layers, weights, biases):\n",
    "    error_on_layers = [_error_on_output_layer(target_data, layers[-1])]\n",
    "    for l, w in zip(layers[-2::-1], weights[:0:-1]):\n",
    "        error_on_layers.append(_error_on_hidden_layers(error_on_layers[-1], l, w))\n",
    "        \n",
    "    biases_error = np.array([_derivative_of_cost_with_respect_to_biases(error) for error in error_on_layers], dtype=object)\n",
    "    weights_error = np.array([_derivative_of_cost_with_respect_to_weights(error, layer) for error, layer in zip(error_on_layers, layers[-2::-1])], dtype=object)\n",
    "\n",
    "    return (weights_error, biases_error)\n",
    "\n",
    "def update_weights_biases(alpha, weights, weights_error, biases, biases_error):    \n",
    "    weights = [w + (alpha * w_e) for w, w_e in zip(weights, weights_error[::-1])]\n",
    "    biases = [b + (alpha * b_e) for b, b_e in zip(biases, biases_error[::-1])]\n",
    "    \n",
    "    return (weights, biases)\n",
    "\n",
    "def train_determinsitic_neural_network(input_layer, hidden_layers, output_layer, feature_data, target_data, alpha, n_iter):\n",
    "    all_layers = combine_all_layers(input_layer, hidden_layers, output_layer)\n",
    "    weights = generate_all_initial_weights(all_layers)\n",
    "    biases = generate_all_initial_biases(all_layers)\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        neuron_values = feed_forward__neural_network(weights, biases, feature_data)\n",
    "        print(neuron_values[-1])\n",
    "        weights_error, biases_error = backward_propagation_neural_network(target_data, neuron_values, weights, biases)\n",
    "        weights, biases = update_weights_biases(0.01, weights, weights_error, biases, biases_error)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd29af5-4e64-43ad-a806-7d9c0b184e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = [1]\n",
    "hidden_layers = [2, 3]\n",
    "output_layer = [1]\n",
    "\n",
    "feature_data = np.array([[1]])\n",
    "target_data = np.array([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88303a1f-7d07-4e4d-864f-f23903212b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.02864932]]\n",
      "[[1.04388684]]\n",
      "[[1.05902143]]\n",
      "[[1.07405172]]\n",
      "[[1.08897629]]\n",
      "[[1.10379367]]\n",
      "[[1.11850238]]\n",
      "[[1.13310087]]\n",
      "[[1.14758761]]\n",
      "[[1.16196101]]\n",
      "[[1.17621948]]\n",
      "[[1.19036143]]\n",
      "[[1.20438522]]\n",
      "[[1.21828925]]\n",
      "[[1.23207189]]\n",
      "[[1.24573153]]\n",
      "[[1.25926655]]\n",
      "[[1.27267538]]\n",
      "[[1.28595642]]\n",
      "[[1.29910813]]\n"
     ]
    }
   ],
   "source": [
    "train_determinsitic_neural_network(input_layer, hidden_layers, output_layer, feature_data, target_data, 0.01, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680364a5-f917-4176-9b58-26a90dcf492d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
